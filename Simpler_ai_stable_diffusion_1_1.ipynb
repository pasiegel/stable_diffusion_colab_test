{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasiegel/stable_diffusion_colab_test/blob/main/Simpler_ai_stable_diffusion_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### Setup\n",
        "\n",
        "First, Click the play button to run go through initial setup. Green checkbox will appear near play button when complete then scroll down to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHkHsdtnry57"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)  \n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "def dummy(images, **kwargs): return images, False \n",
        "pipe.safety_checker = dummy\n",
        "\n",
        "import torch\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSTsT6McuzWW"
      },
      "source": [
        "Generate images by adjusting the \"Prompt = \" Below:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "example prompt: prompt = \"Best quality, illustration, detailed, 4k, Terminator, Robot , Cyborg, perfect face\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "prompt = \"Best quality, illustration, detailed, 4k, crypt keeper\"\n",
        "image = pipe(prompt).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZCwCecVJI3"
      },
      "source": [
        "Running the above cell multiple times will give you a different image every time. If you want deterministic output you can pass a random seed to the pipeline. Every time you use the same seed you'll have the same image result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaAW4sSdV7vZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(5000)\n",
        "\n",
        "image = pipe(prompt, generator=generator).images[0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrbYQVQXK6I"
      },
      "source": [
        "You can change the number of inference steps using the `num_inference_steps` argument. In general, results are better the more steps you use. Stable Diffusion, being one of the latest models, works great with a relatively small number of steps, so we recommend to use the default of `50`. If you want faster results you can use a smaller number.\n",
        "\n",
        "The following cell uses the same seed as before, but with fewer steps. Note how some details, such as the horse's head or the helmet, are less defin realistic and less defined than in the previous image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKiK67iTXQkt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(500)\n",
        "\n",
        "image = pipe(prompt, num_inference_steps=50, generator=generator).images[0]\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8wxFjba5zRc"
      },
      "source": [
        "The other parameter in the pipeline call is `guidance_scale`. It is a way to increase the adherence to the conditional signal which in this case is text as well as overall sample quality. In simple terms classifier free guidance forces the generation to better match with the prompt. Numbers like `7` or `8.5` give good results, if you use a very large number the images might look good, but will be less diverse. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbR3IszB1CD"
      },
      "source": [
        "To generate multiple images for the same prompt, we simply use a list with the same prompt repeated several times. We'll send the list to the pipeline instead of the string we used before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZcgsflpBoEM"
      },
      "source": [
        "\n",
        "\n",
        "Let's first write a helper function to display a grid of images. Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "REF_yuHprSa1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcHccTDWbQRU"
      },
      "source": [
        "Now, we can generate a grid image once having run the pipeline with a list of 3 prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAFLvWWrSdM"
      },
      "outputs": [],
      "source": [
        "num_images = 3\n",
        "prompt = [\"Best quality, illustration, detailed, 4k, crypt keeper\"] * num_images\n",
        "\n",
        "images = pipe(prompt).images\n",
        "\n",
        "grid = image_grid(images, rows=1, cols=3)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-3lCCWYtMn"
      },
      "source": [
        "And here's how to generate a grid of `n × m` images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylscg48YYxfF"
      },
      "outputs": [],
      "source": [
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "prompt = [\"Best quality, illustration, detailed, 4k, crypt keeper\"] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9pbS3kCsUf"
      },
      "source": [
        "### Generate non-square images\n",
        "\n",
        "Stable Diffusion produces images of `512 × 512` pixels by default. But it's very easy to override the default using the `height` and `width` arguments, so you can create rectangular images in portrait or landscape ratios.\n",
        "\n",
        "These are some recommendations to choose good image sizes:\n",
        "- Make sure `height` and `width` are both multiples of `8`.\n",
        "- Going below 512 might result in lower quality images.\n",
        "- Going over 512 in both directions will repeat image areas (global coherence is lost).\n",
        "- The best way to create non-square images is to use `512` in one dimension, and a value larger than that in the other one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SXnxd-ZrSfy"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = \"Best quality, illustration, detailed, 4k, crypt keeper\"\n",
        "\n",
        "image = pipe(prompt, height=512, width=768).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the parameters we'll use to generate images.\n",
        "\n",
        "Note that guidance_scale is defined analog to the guidance weight w of equation (2) in the Imagen paper. guidance_scale == 1 corresponds to doing no classifier-free guidance. Here we set it to 7.5 as also done previously.\n",
        "\n",
        "In contrast to the previous examples, we set num_inference_steps to 100 to get an even more defined image."
      ],
      "metadata": {
        "id": "IpcE2E5fxS2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"Best quality, illustration, detailed, 4k, crypt keeper\"]\n",
        "\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "\n",
        "num_inference_steps = 100            # Number of denoising steps\n",
        "\n",
        "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
        "\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "  max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   \n",
        "\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "\n",
        "latents.shape\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "latents = latents * scheduler.init_noise_sigma\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
        "  # scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "LB9yVfpMxQNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}